<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="google-site-verification" content="bxKspdX9aHnO2BI-bVWFh3THpvK-0IF27YTgeSXX6PI" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta name="application-name" content="Xingchen Zeng" />
  <title>Xingchen Zeng</title>
  <meta name="description" content="Xingchen Zeng - PhD Student in Data Science and Analytics" />
  <link rel="shortcut icon" href="img/star.svg" type="image/x-icon" />
  <link rel="icon" href="img/star.svg" type="image/x-icon" />
  <meta name="keywords" content="Xingchen Zeng, Data Science, Visualization, Large Language Models, HKUST" />
  <meta name="author" content="Xingchen Zeng" />
  <link href="css/custom.css" rel="stylesheet" />
</head>

<body>
  <div class="container">
    <!-- About Section -->
    <section id="about" class="section-first">
      <div class="about-content">
        <div class="avatar-wrapper">
          <img src="img/avatar.jpg" alt="Xingchen Zeng" class="avatar" />
        </div>
        <h1 class="name">Xingchen Zeng <span class="chinese-name">(曾星辰)</span></h1>
        <p>
          I am a third-year PhD student in Data Science and Analytics at the 
          <strong>Hong Kong University of Science and Technology (Guangzhou)</strong>, 
          supervised by <a href="https://www.zeng-wei.com">Prof. Wei Zeng</a> and 
          <a href="https://cse.hkust.edu.hk/~weiwcs/">Prof. Wei Wang</a>. 
          I received my Bachelor's degree from <strong>Central South University</strong> with honors, 
          supervised by <a href="https://www.xiajiazhi.com">Prof. Jiazhi Xia</a>.
        </p>
        <p class="research-interests">
          My research focuses on empowering Large Language Models</strong> with human-like 
          comprehension and creation of <strong> structured visual content</strong>, unlocking new capabilities 
          in AI-driven design, reasoning, and communication.
        </p>
        <p class="contact">
          Email: <a href="mailto:xingchen.zeng@outlook.com" class="email">xingchen.zeng@outlook.com</a><br>
          Links: [<a href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank">Google Scholar</a>] 
          [<a href="https://github.com/zengxingchen" target="_blank">GitHub</a>]
          [<a href="files/cv.pdf" target="_blank">Curriculum Vitae</a>]
        </p>
        <p class="note">
          I'm currently seeking research internship opportunities. If you're interested in working together or can recommend someone who might be a good fit, I'd greatly appreciate it!
        </p>
      </div>
    </section>

    <!-- News Section -->
    <section id="news" class="section">
      <h2>News</h2>
      <ul class="news-list">
        <li>
          <span class="date">2025-05</span>
          <em>One paper accepted by ICML 2025!</em> Congrats to Yilin!
        </li>
        <li>
          <span class="date">2025-03</span>
          <em>One paper accepted by CHI 2025!</em> Congrats to Yihan! 
          We contribute a robust data-mining framework for quantifying color semantics 
          learned by pretrained image generation models.
        </li>
        <li>
          <span class="date">2024-10</span>
          Check the <a href="https://github.com/zengxingchen/LLM-Visualization-Paper-List">
          <strong>LLM × Visualization</strong></a> paper list. Feel free to open an issue 
          or pull request to add papers you appreciate.
        </li>
        <li>
          <span class="date">2024-07</span>
          <em>Two papers accepted by VIS 2024!</em> Check our preprints and codes!
        </li>
      </ul>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="section">
      <h2>Publications</h2>
      
      <!-- Under Review -->
      <div class="subsection">
        <h3>Under Review</h3>
        <ol class="paper-list">
          <li>
            <strong>Xingchen Zeng</strong>, Zhewei Su, Hengming Zhang, Juyong Jiang, Jiazhi Xia, Wei Zeng. 
            <span class="paper-title">DaVinci: Reinforcing Visual-Structural Syntax in MLLMs for 
            Generalized Scientific Diagram Parsing.</span> 
            Submitted to ICLR 2026. 
            <a href="https://openreview.net/pdf?id=OAXECnLxuk" class="paper-link">Preprint</a>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Yuanbang Liu, Jianing Hao, Wei Zeng. 
            <span class="paper-title">Chart-G1: Visually Grounded Chart Reasoning by Rewarding 
            Multimodal Large Language Models.</span> 
            Submitted to PacificVis 2026 TVCG Track.
          </li>
        </ol>
      </div>

      <!-- Published Papers -->
      <div class="subsection">
        <h3>Published</h3>
        <ol class="paper-list">
          <li>
            <strong>Xingchen Zeng</strong>, Haichuan Lin, Yilin Ye, Wei Zeng. 
            <span class="paper-title">Advancing Multimodal Large Language Models in Chart Question 
            Answering with Visualization-Referenced Instruction Tuning.</span> 
            <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 2024. (CCF Rank: A)
            <a href="https://arxiv.org/abs/2407.20174" class="paper-link">Paper</a>
            <a href="https://github.com/zengxingchen/ChartQA-MLLM" class="paper-link">Code</a>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Ziyao Gao, Yilin Ye, Wei Zeng. 
            <span class="paper-title">IntentTuner: An Interactive Framework for Integrating Human 
            Intentions in Fine-tuning Text-to-Image Generative Models.</span> 
            <em>Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, 2024. (CCF Rank: A)
            <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642165" class="paper-link">Paper</a>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Haowen Zhou, Zhicong Li, Chenqi Zhang, Juncong Lin, 
            Jiazhi Xia, Yanyi Yang, Xiaoyan Kui. 
            <span class="paper-title">iHELP: interactive hierarchical linear projections for 
            interpreting non-linear projections.</span> 
            <em>Journal of Visualization (Proc. ChinaVis 2022)</em>, 26(3): 631-648, 2023.
            <a href="https://link.springer.com/article/10.1007/s12650-022-00900-4" class="paper-link">Paper</a>
          </li>
          <li>
            Yilin Ye, Junchao Huang, <strong>Xingchen Zeng</strong>, Jiazhi Xia, Wei Zeng. 
            <span class="paper-title">Parametric-Generalized-Kernel Boosted Dimensionality Reduction 
            for Gaussian Kernel Estimation in Projection Space.</span> 
            <em>Proceedings of the Forty-Second International Conference on Machine Learning</em>, 2025. 
            <a href="https://arxiv.org/pdf/2503.03236" class="paper-link">Paper</a> <a href="https://github.com/yilinye/AKRMap" class="paper-link">Code</a> 
          </li>
          <li>
            Yihan Hou, <strong>Xingchen Zeng</strong>, Yusong Wang, Manling Yang, Xiaojiao Chen, Wei Zeng. 
            <span class="paper-title">GenColor: Generative Color-Concept Association in Visual Design.</span> 
            <em>Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, 2025.
            <a href="https://arxiv.org/pdf/2503.03236" class="paper-link">Paper</a> <a href="https://github.com/Sunnary2604/GenColor" class="paper-link">Code</a>
          </li>
          <li>
            Yilin Ye, Shishi Xiao, <strong>Xingchen Zeng</strong>, Wei Zeng. 
            <span class="paper-title">ModalChorus: Visual Probing and Alignment of Multi-modal 
            Embeddings via Modal Fusion Map.</span> 
            <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 2024.
            <a href="https://arxiv.org/abs/2407.12315" class="paper-link">Paper</a> <a href="https://github.com/yilinye/Modal-Fusion-Map" class="paper-link">Code</a>
          </li>
        </ol>
      </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
      <p>© 2025 Xingchen Zeng. Last updated October 2025.</p>
    </footer>
  </div>
</body>

</html>