<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="google-site-verification" content="bxKspdX9aHnO2BI-bVWFh3THpvK-0IF27YTgeSXX6PI" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta name="application-name" content="Xingchen Zeng" />
  <title>Xingchen Zeng</title>
  <meta name="description" content="Xingchen Zeng - PhD Student in Data Science and Analytics" />
  <link rel="shortcut icon" href="img/star.svg" type="image/x-icon" />
  <link rel="icon" href="img/star.svg" type="image/x-icon" />
  <meta name="keywords" content="Xingchen Zeng, Data Science, Visualization, Large Language Models, HKUST" />
  <meta name="author" content="Xingchen Zeng" />
  <link href="css/custom.css" rel="stylesheet" />

</head>

<body>
  <div class="container">
    <!-- About Section -->
    <section id="about" class="section-first">
      <div class="about-content">
        <div class="avatar-wrapper">
          <img src="img/avatar.jpg" alt="Xingchen Zeng" class="avatar" />
        </div>
        <div class="bio-content">
          <h1 class="name">Xingchen Zeng <span class="chinese-name-wrapper"><img src="img/chinese_name.svg" alt="曾星辰" class="chinese-name-img"></span></h1>
          <p>
            I am a third-year PhD student in Data Science and Analytics at the 
            <strong>Hong Kong University of Science and Technology (Guangzhou)</strong>, 
            supervised by <a href="https://www.zeng-wei.com">Prof. Wei Zeng</a> and 
            <a href="https://cse.hkust.edu.hk/~weiwcs/">Prof. Wei Wang</a>. 
            I received my Bachelor's degree from <strong>Central South University</strong> with honors, 
            supervised by <a href="https://www.xiajiazhi.com">Prof. Jiazhi Xia</a>.
          </p>
          <p class="research-interests">
            My research focuses on empowering Large Language Models</strong> with human-like 
            comprehension and creation of <strong> structured visual content</strong>, unlocking new capabilities 
            in AI-driven design, reasoning, and communication.
          </p>
          <p class="contact">
            Email: <a href="mailto:xingchen.zeng@outlook.com" class="email">xingchen.zeng@outlook.com</a><br>
            Links: [<a href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank">Google Scholar</a>] 
            [<a href="https://github.com/zengxingchen" target="_blank">GitHub</a>]
            [<a href="file/CV-Xingchen-General.pdf" target="_blank">Curriculum Vitae</a>]
          </p>
        </div>
      </div>
      <p class="note">
        I'm currently seeking research internship opportunities. If you're interested in working together or can recommend someone who might be a good fit, I'd greatly appreciate it!
      </p>
    </section>

    <!-- News Section -->
    <section id="news" class="section">
      <h2>News</h2>
      <ul class="news-list">
        <li>
          <span class="date">2025-05</span>
          <em>One paper accepted by ICML 2025!</em> Congrats to Yilin!
        </li>
        <li>
          <span class="date">2025-03</span>
          <em>One paper accepted by CHI 2025!</em> Congrats to Yihan! 
          We contribute a robust data-mining framework for quantifying color semantics 
          learned by pretrained image generation models.
        </li>
        <li>
          <span class="date">2024-10</span>
          Check the <a href="https://github.com/zengxingchen/LLM-Visualization-Paper-List" class="common-link">
          LLM × Visualization</strong></a> paper list. Feel free to open an issue 
          or pull request to add papers you appreciate.
        </li>
        <!-- <li>
          <span class="date">2024-07</span>
          <em>Two papers accepted by VIS 2024!</em> Check our preprints and codes!
        </li> -->
      </ul>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="section">
      <h2>Papers</h2>
      
      <!-- Under Review -->
      <div class="subsection">
        <h3>Under Review</h3>
        <ol class="paper-list">
          <li>
            <strong>Xingchen Zeng</strong>, Zhewei Su, Hengming Zhang, Juyong Jiang, Jiazhi Xia, Wei Zeng. 
            <span class="paper-title">DaVinci: Reinforcing Visual-Structural Syntax in MLLMs for 
            Generalized Scientific Diagram Parsing.</span> 
            Submitted to ICLR 2026. 
            <a href="https://openreview.net/pdf?id=OAXECnLxuk" class="paper-link">Preprint</a>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Yuanbang Liu, Jianing Hao, Wei Zeng. 
            <span class="paper-title">Chart-G1: Visually Grounded Chart Reasoning by Rewarding 
            Multimodal Large Language Models.</span> 
            Submitted to PacificVis 2026 TVCG Track.
          </li>
        </ol>
      </div>

      <!-- Published Papers -->
      <div class="subsection">
        <h3>Published</h3>
        <ol class="paper-list">
          <li>
            <strong>Xingchen Zeng</strong>, Haichuan Lin, Yilin Ye, Wei Zeng. 
            <span class="paper-title">Advancing Multimodal Large Language Models in Chart Question 
            Answering with Visualization-Referenced Instruction Tuning.</span> 
            <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 2024.
            <a href="https://arxiv.org/abs/2407.20174" class="paper-link">Paper</a>
            <a href="https://github.com/zengxingchen/ChartQA-MLLM" class="paper-link">Code</a>
            <a class="abstract-toggle" onclick="toggleAbstract(this)">
              <span>Abstract</span>
              <span class="arrow">▼</span>
            </a>
            <div class="abstract-content">
              <div class="abstract-text">
                Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (i.e., charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research.
              </div>
            </div>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Ziyao Gao, Yilin Ye, Wei Zeng. 
            <span class="paper-title">IntentTuner: An Interactive Framework for Integrating Human 
            Intentions in Fine-tuning Text-to-Image Generative Models.</span> 
            <em>Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, 2024. 
            <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642165" class="paper-link">Paper</a>
          </li>
          <li>
            <strong>Xingchen Zeng</strong>, Haowen Zhou, Zhicong Li, Chenqi Zhang, Juncong Lin, 
            Jiazhi Xia, Yanyi Yang, Xiaoyan Kui. 
            <span class="paper-title">iHELP: interactive hierarchical linear projections for 
            interpreting non-linear projections.</span> 
            <em>Journal of Visualization (Proc. ChinaVis 2022)</em>, 26(3): 631-648, 2023.
            <a href="https://link.springer.com/article/10.1007/s12650-022-00900-4" class="paper-link">Paper</a>
          </li>
          <li>
            Yilin Ye, Junchao Huang, <strong>Xingchen Zeng</strong>, Jiazhi Xia, Wei Zeng. 
            <span class="paper-title">Parametric-Generalized-Kernel Boosted Dimensionality Reduction 
            for Gaussian Kernel Estimation in Projection Space.</span> 
            <em>Proceedings of the Forty-Second International Conference on Machine Learning</em>, 2025. 
            <a href="https://arxiv.org/pdf/2503.03236" class="paper-link">Paper</a>
            <a href="https://github.com/yilinye/AKRMap" class="paper-link">Code</a>
          </li>
          <li>
            Yihan Hou, <strong>Xingchen Zeng</strong>, Yusong Wang, Manling Yang, Xiaojiao Chen, Wei Zeng. 
            <span class="paper-title">GenColor: Generative Color-Concept Association in Visual Design.</span> 
            <em>Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, 2025.
            <a href="https://arxiv.org/pdf/2503.03236" class="paper-link">Paper</a>
            <a href="https://github.com/Sunnary2604/GenColor" class="paper-link">Code</a>
          </li>
          <li>
            Yilin Ye, Shishi Xiao, <strong>Xingchen Zeng</strong>, Wei Zeng. 
            <span class="paper-title">ModalChorus: Visual Probing and Alignment of Multi-modal 
            Embeddings via Modal Fusion Map.</span> 
            <em>IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE VIS 2024)</em>, 2024.
            <a href="https://arxiv.org/abs/2407.12315" class="paper-link">Paper</a>
            <a href="https://github.com/yilinye/Modal-Fusion-Map" class="paper-link">Code</a>
          </li>
        </ol>
      </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
      <p>© 2025 Xingchen Zeng. Last updated October 2025.</p>
    </footer>
  </div>

  <script>
    function toggleAbstract(element) {
      // 切换按钮的expanded类
      element.classList.toggle('expanded');
      
      // 找到对应的abstract内容
      const abstractContent = element.nextElementSibling;
      
      // 切换内容的展开/收起状态
      abstractContent.classList.toggle('expanded');
    }
  </script>
</body>

</html>